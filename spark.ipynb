{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T23:53:29.636424Z",
     "iopub.status.busy": "2021-01-26T23:53:29.636424Z",
     "iopub.status.idle": "2021-01-26T23:53:30.148394Z",
     "shell.execute_reply": "2021-01-26T23:53:30.148394Z",
     "shell.execute_reply.started": "2021-01-26T23:53:29.636424Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T01:53:02.284698Z",
     "iopub.status.busy": "2021-01-27T01:53:02.284698Z",
     "iopub.status.idle": "2021-01-27T01:53:02.293700Z",
     "shell.execute_reply": "2021-01-27T01:53:02.293700Z",
     "shell.execute_reply.started": "2021-01-27T01:53:02.284698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chaon\\anaconda3\\envs\\spark\\python.exe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://MIMI-PC:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f5e8e261f0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print(sys.executable)\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "#     .master('local[2]')\\\n",
    "#     .appName('Hierarchical data') \\\n",
    "    .config('spark.sql.execution.arrow.pyspark.enabled', 'true')\n",
    "    .config('spark.sql.shuffle.partitions', 4)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate on hierarchical data\n",
    "\n",
    "Steps:\n",
    "1. extract hierarchy into dict, where key is child and value is a list containing itself, and its parent and grand-parents\n",
    "2. replace child id with its parent list using udf\n",
    "3. explode\n",
    "4. do aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T23:53:45.011606Z",
     "iopub.status.busy": "2021-01-26T23:53:45.011606Z",
     "iopub.status.idle": "2021-01-26T23:53:45.026627Z",
     "shell.execute_reply": "2021-01-26T23:53:45.025606Z",
     "shell.execute_reply.started": "2021-01-26T23:53:45.011606Z"
    }
   },
   "outputs": [],
   "source": [
    "hier = {\n",
    "    1: [1],\n",
    "    2: [2, 1],\n",
    "    3: [3, 1],\n",
    "    4: [4, 2, 1],\n",
    "    5: [5, 3, 1],\n",
    "    6: [6, 4, 2, 1],\n",
    "    7: [7, 5, 3, 1],\n",
    "    8: [8, 6, 4, 2, 1],\n",
    "    9: [9, 5, 3, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T23:53:45.027606Z",
     "iopub.status.busy": "2021-01-26T23:53:45.027606Z",
     "iopub.status.idle": "2021-01-26T23:53:45.057608Z",
     "shell.execute_reply": "2021-01-26T23:53:45.056606Z",
     "shell.execute_reply.started": "2021-01-26T23:53:45.027606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>attr</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>f1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>f2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>f1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>f2</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>f1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>f2</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entity_id attr  value\n",
       "0          7   f1      1\n",
       "1          7   f2      5\n",
       "2          8   f1     10\n",
       "3          8   f2     50\n",
       "4          9   f1    100\n",
       "5          9   f2    500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data={\n",
    "    'entity_id': [7, 7, 8, 8, 9, 9],\n",
    "    'attr': ['f1', 'f2', 'f1', 'f2', 'f1', 'f2'],\n",
    "    'value': [1, 5, 10, 50, 100, 500]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T23:53:45.058629Z",
     "iopub.status.busy": "2021-01-26T23:53:45.058629Z",
     "iopub.status.idle": "2021-01-26T23:53:46.638605Z",
     "shell.execute_reply": "2021-01-26T23:53:46.637605Z",
     "shell.execute_reply.started": "2021-01-26T23:53:45.058629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- entity_id: long (nullable = true)\n",
      " |-- attr: string (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd = spark.createDataFrame(df)\n",
    "dd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T23:53:46.639606Z",
     "iopub.status.busy": "2021-01-26T23:53:46.639606Z",
     "iopub.status.idle": "2021-01-26T23:53:46.841620Z",
     "shell.execute_reply": "2021-01-26T23:53:46.840615Z",
     "shell.execute_reply.started": "2021-01-26T23:53:46.639606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- entity_id: string (nullable = true)\n",
      " |-- attr: string (nullable = true)\n",
      " |-- sum(value): long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql.types import *\n",
    "from typing import List\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def to_array(x: int) -> List[int]:\n",
    "    return hier.get(x, [])\n",
    "\n",
    "dd_new = (\n",
    "    dd\n",
    "    .withColumn('entity_id', to_array('entity_id'))\n",
    "    .withColumn('entity_id', explode('entity_id'))\n",
    "    .groupBy('entity_id', 'attr')\n",
    "    .agg({'value': 'sum'})\n",
    ")\n",
    "dd_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T23:53:46.843606Z",
     "iopub.status.busy": "2021-01-26T23:53:46.843606Z",
     "iopub.status.idle": "2021-01-26T23:53:51.726699Z",
     "shell.execute_reply": "2021-01-26T23:53:51.725699Z",
     "shell.execute_reply.started": "2021-01-26T23:53:46.843606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+\n",
      "|entity_id|attr|sum(value)|\n",
      "+---------+----+----------+\n",
      "|        3|  f1|       101|\n",
      "|        7|  f2|         5|\n",
      "|        3|  f2|       505|\n",
      "|        6|  f2|        50|\n",
      "|        2|  f2|        50|\n",
      "|        9|  f1|       100|\n",
      "|        7|  f1|         1|\n",
      "|        5|  f1|       101|\n",
      "|        5|  f2|       505|\n",
      "|        4|  f1|        10|\n",
      "|        9|  f2|       500|\n",
      "|        1|  f1|       111|\n",
      "|        8|  f2|        50|\n",
      "|        1|  f2|       555|\n",
      "|        8|  f1|        10|\n",
      "|        6|  f1|        10|\n",
      "|        2|  f1|        10|\n",
      "|        4|  f2|        50|\n",
      "+---------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D interpolation\n",
    "## Fixed NA columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T01:53:09.025000Z",
     "iopub.status.busy": "2021-01-27T01:53:09.025000Z",
     "iopub.status.idle": "2021-01-27T01:53:09.047045Z",
     "shell.execute_reply": "2021-01-27T01:53:09.047045Z",
     "shell.execute_reply.started": "2021-01-27T01:53:09.025000Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0.3, 0.6, 1. ],\n",
       "       [1. , 0.7, 0.4, 0. ]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample using numpy and scipy\n",
    "x = np.array(['0', '0.3', '0.6', '1.0']).astype(np.float)\n",
    "y = np.array([\n",
    "    [0, np.nan, np.nan, 1],\n",
    "    [1, np.nan, np.nan, 0],\n",
    "])\n",
    "cond = np.isnan(y[0])\n",
    "yy = y[:, ~cond]\n",
    "xx = x[~cond]\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "f = interp1d(xx, yy)\n",
    "y[:, cond] =  f(x[cond])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T01:53:09.384369Z",
     "iopub.status.busy": "2021-01-27T01:53:09.384369Z",
     "iopub.status.idle": "2021-01-27T01:53:09.391353Z",
     "shell.execute_reply": "2021-01-27T01:53:09.390352Z",
     "shell.execute_reply.started": "2021-01-27T01:53:09.384369Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# @pandas_udf()\n",
    "# def interp_1d(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "#     y = pdf.values\n",
    "#     x = np.array(pdf.columns).astype(np.float)\n",
    "#     cond = np.isnan(y[0])\n",
    "#     yy = y[:, ~cond]\n",
    "#     xx = x[~cond]\n",
    "    \n",
    "#     from scipy.interpolate import interp1d\n",
    "#     f = interp1d(xx, yy)\n",
    "#     y[:, cond] = f(x[cond])\n",
    "#     return pd.DataFrame.from_records(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T02:49:09.169236Z",
     "iopub.status.busy": "2021-01-27T02:49:09.168235Z",
     "iopub.status.idle": "2021-01-27T02:49:09.176236Z",
     "shell.execute_reply": "2021-01-27T02:49:09.176236Z",
     "shell.execute_reply.started": "2021-01-27T02:49:09.169236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test \n",
    "columns = ['0', '30', '60', '100']\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    columns=['book_id'] + columns,\n",
    "    data=[  \n",
    "        ['a',  0.0, np.nan, np.nan, 100],\n",
    "        ['b',  1.0, np.nan, np.nan, 0.0],\n",
    "        ['c',  0.0, np.nan, np.nan, -100],\n",
    "        ['d',  -1.0, np.nan, np.nan, 0.0],\n",
    "     ])\n",
    "\n",
    "df.dtypes == np.floating\n",
    "# interp_1d.func(df[columns])\n",
    "df.select_dtypes(include=np.floating).columns.isin(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T02:49:10.784485Z",
     "iopub.status.busy": "2021-01-27T02:49:10.784485Z",
     "iopub.status.idle": "2021-01-27T02:49:13.268607Z",
     "shell.execute_reply": "2021-01-27T02:49:13.267643Z",
     "shell.execute_reply.started": "2021-01-27T02:49:10.784485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- book_id: string (nullable = true)\n",
      " |-- 0: double (nullable = true)\n",
      " |-- 30: double (nullable = true)\n",
      " |-- 60: double (nullable = true)\n",
      " |-- 100: double (nullable = true)\n",
      "\n",
      "+-------+----+-----+-----+------+\n",
      "|book_id|   0|   30|   60|   100|\n",
      "+-------+----+-----+-----+------+\n",
      "|      d|-1.0| -0.7| -0.4|   0.0|\n",
      "|      a| 0.0| 30.0| 60.0| 100.0|\n",
      "|      b| 1.0|  0.7|  0.4|   0.0|\n",
      "|      c| 0.0|-30.0|-60.0|-100.0|\n",
      "+-------+----+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct, col\n",
    "\n",
    "dd = spark.createDataFrame(df)\n",
    "dd = dd.repartition(3)\n",
    "\n",
    "def interp_1d(it):\n",
    "    # assume each column is either all nan or all valid\n",
    "    \n",
    "    for pdf in it:\n",
    "        # select floating columns\n",
    "        df = pdf.select_dtypes(include=np.floating)\n",
    "        columns = df.columns.tolist()\n",
    "        \n",
    "        # filter out nan columns\n",
    "        y = df.values\n",
    "        x = np.array(columns).astype(np.float)\n",
    "        cond = np.isnan(y[0])\n",
    "        yy = y[:, ~cond]\n",
    "        xx = x[~cond]\n",
    "\n",
    "        from scipy.interpolate import interp1d\n",
    "        f = interp1d(xx, yy)\n",
    "        y[:, cond] = f(x[cond])\n",
    "        \n",
    "        new_df = pd.DataFrame.from_records(y, columns=columns)\n",
    "        \n",
    "        df = pd.concat([new_df, pdf.select_dtypes(exclude=np.floating)], axis=1)\n",
    "        \n",
    "        yield df.reindex(columns=pdf.columns.tolist())\n",
    "        \n",
    "dd_new = dd.mapInPandas(interp_1d, dd.schema)\n",
    "\n",
    "dd_new.printSchema()\n",
    "dd_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    columns=['book_id',  '0/0', '0/10', '10/0', '10/10'],\n",
    "    data=[  ['a',  0.0, 10.0, 100.0, np.nan] ])\n",
    "\n",
    "dd = spark.createDataFrame(df)\n",
    "dd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "@pandas_udf('double')\n",
    "def interp_2d(s: pd.Series) -> pd.Series:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "tup = [(1, \"a\"), (3, \"a\"), (2, \"a\"), (1, \"b\"), (2, \"b\"), (3, \"b\")]\n",
    "df = sqlContext.createDataFrame(tup, [\"id\", \"category\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy(\"category\").orderBy(\"id\").rangeBetween(Window.currentRow, 1)\n",
    "df.withColumn(\"sum\", func.sum(\"id\").over(window)).show()\n",
    "# df.withColumn(\"sum\", func.sum(\"id\").over(window)).sort(\"id\", \"category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window = Window.partitionBy(\"category\").orderBy(\"id\").rowsBetween(-1, Window.currentRow)\n",
    "window = Window.partitionBy(\"category\").rowsBetween(-1, Window.currentRow)\n",
    "df.withColumn(\"sum\", func.sum(\"id\").over(window)).show()\n",
    "# df.withColumn(\"sum\", func.sum(\"id\").over(window)).sort(\"id\", \"category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
